{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'capability', 'task', 'instance_idx', 'system_prompt', 'input',\n",
       "       'reference_answer', 'score_rubric', 'response', 'uuid', 'model_name',\n",
       "       'used_for_training', 'human_score', 'language', 'prometheus_8x7b_score',\n",
       "       'prometheus_8x7b_feedback', 'prometheus_8x7b_bgb_score',\n",
       "       'prometheus_8x7b_bgb_feedback', 'gpt4_score', 'gpt4_feedback',\n",
       "       'gpt4_04_turbo_score', 'gpt4_04_turbo_feedback', 'claude_score',\n",
       "       'claude_feedback', '__index_level_0__', 'principle_raw',\n",
       "       'principle_raw_prompt', 'iteration_principle', 'principle_final',\n",
       "       'principles_critique_iter_1', 'principles_critique_iter_1_prompt',\n",
       "       'principles_critique_final',\n",
       "       'formatted_BigGenBench_INIT_GRADING_PROMPT', 'init_response',\n",
       "       'init_response_prompt', 'init_response-[score]',\n",
       "       'principle_refined_iter_1', 'principle_refined_iter_1_prompt',\n",
       "       'principles_critique_iter_2', 'principles_critique_iter_2_prompt',\n",
       "       'principle_refined_iter_2', 'principle_refined_iter_2_prompt',\n",
       "       'principles_critique_iter_3', 'principles_critique_iter_3_prompt',\n",
       "       'error_in_principle_critique_iter_1',\n",
       "       'error_in_principle_critique_iter_2', 'principle_refined_iter_3',\n",
       "       'principle_refined_iter_3_prompt', 'principles_critique_iter_4',\n",
       "       'principles_critique_iter_4_prompt',\n",
       "       'error_in_principle_critique_iter_4',\n",
       "       'error_in_principle_critique_iter_3', 'iteration_response',\n",
       "       'response_final', 'response_final-[score]', 'response_critique_iter_1',\n",
       "       'response_critique_iter_1_prompt', 'response_refined_iter_1',\n",
       "       'response_refined_iter_1_prompt', 'response_critique_iter_2',\n",
       "       'response_critique_iter_2_prompt', 'response_refined_iter_2',\n",
       "       'response_refined_iter_2_prompt', 'response_critique_iter_3',\n",
       "       'response_critique_iter_3_prompt', 'response_refined_iter_3',\n",
       "       'response_refined_iter_3_prompt', 'response_critique_iter_4',\n",
       "       'response_critique_iter_4_prompt', 'response_critique_final',\n",
       "       'error_in_response_critique_iter_3',\n",
       "       'error_in_response_critique_iter_1',\n",
       "       'error_in_response_critique_iter_4',\n",
       "       'error_in_response_critique_iter_2'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "model_name = \"meta-llama/llama-3-1-70b-instruct\"\n",
    "\n",
    "# file_path = f\"./outputs/{model_name}/principle_instruct-[2024_11_28]-[reference_free]-[no_seed_examples]-[04_responses_refined].jsonl\"\n",
    "file_path = f\"./outputs/{model_name}/principle_instruct-[2024_11_28]-[reference_free]-[6_default_principles]-[04_responses_refined].jsonl\"\n",
    "# file_path = f\"./outputs/{model_name}/principle_instruct-[2024_11_28]-[reference_free]-[3_for_each_capacity]-[04_responses_refined].jsonl\"\n",
    "\n",
    "\n",
    "file_df = pd.read_json(file_path, lines=True)\n",
    "file_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mmeta-llama/llama-3-1-70b-instruct\u001b[0m\n",
      "\u001b[32mprinciple_instruct-[2024_11_28]-[reference_free]-[6_default_principles]-[04_responses_refined].jsonl\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.1410071942446045"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "principle_instruct_columns = [\"principle_raw\"] + [col for col in file_df.columns if col.startswith(\"principles_critique_iter\") and not col.endswith(\"prompt\")] + [col for col in file_df.columns if col.startswith(\"principle_refined_iter\") and not col.endswith(\"prompt\")] + [\"init_response\"] + [col for col in file_df.columns if col.startswith(\"response_critique_iter\") and not col.endswith(\"prompt\")] + [col for col in file_df.columns if col.startswith(\"response_refined_iter\") and not col.endswith(\"prompt\")]\n",
    "\n",
    "# display(file_df[principle_instruct_columns].head())\n",
    "\n",
    "from termcolor import colored\n",
    "print (colored(model_name, \"yellow\"))\n",
    "print (colored(file_path.split(\"/\")[-1], \"green\"))\n",
    "\n",
    "import numpy as np\n",
    "# Count non-NaN values in these columns for each row\n",
    "non_nan_counts = file_df[principle_instruct_columns].notna().sum(axis=1)\n",
    "np.mean(non_nan_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
