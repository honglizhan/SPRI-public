{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Reddit ID', 'Reddit Post', 'appraisal_dimension_id',\n",
       "       'appraisal_dimension', 'appraisal_dimension_aim', 'constitution',\n",
       "       'standards', 'principle_raw', 'principle_raw_prompt',\n",
       "       'iteration_principle', 'principle_final', 'principles_critique_iter_1',\n",
       "       'principles_critique_iter_1_prompt', 'principles_critique_final',\n",
       "       'init_response', 'init_response_prompt', 'iteration_response',\n",
       "       'response_final', 'response_critique_iter_1',\n",
       "       'response_critique_iter_1_prompt', 'response_critique_final',\n",
       "       'principle_refined_iter_1', 'principle_refined_iter_1_prompt',\n",
       "       'principles_critique_iter_2', 'principles_critique_iter_2_prompt',\n",
       "       'error_in_principle_critique_iter_2', 'principle_refined_iter_2',\n",
       "       'principle_refined_iter_2_prompt', 'principles_critique_iter_3',\n",
       "       'principles_critique_iter_3_prompt',\n",
       "       'error_in_principle_critique_iter_1', 'response_refined_iter_1',\n",
       "       'response_refined_iter_1_prompt', 'response_critique_iter_2',\n",
       "       'response_critique_iter_2_prompt'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# file_path = \"./outputs-[RESORT_baselines]/ITER/+cons_principle_instruct/unique_resort_eval_data/llama-3-70b-instruct.jsonl\"\n",
    "file_path = \"./outputs-[seed=1_RESORT_constitutions_per_dimension]/mixtral-8x7b-instruct-v01/[resort_human_eval_30]-generated_refined_responses.jsonl\"\n",
    "file_df = pd.read_json(file_path, lines=True)\n",
    "file_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.666666666666667"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reappraisal_columns = [\n",
    "    col for col in file_df.columns if col.startswith(\"reappraisal_output\") or col.startswith(\"intermediate_output\") or col.startswith(\"appraisal_output\")\n",
    "]\n",
    "\n",
    "principle_instruct_columns = [\"principle_raw\"] + [col for col in file_df.columns if col.startswith(\"principles_critique_iter\") and not col.endswith(\"prompt\")] + [col for col in file_df.columns if col.startswith(\"principle_refined_iter\") and not col.endswith(\"prompt\")] + [\"init_response\"] + [col for col in file_df.columns if col.startswith(\"response_critique_iter\") and not col.endswith(\"prompt\")] + [col for col in file_df.columns if col.startswith(\"response_refined_iter\") and not col.endswith(\"prompt\")]\n",
    "\n",
    "# display(file_df[principle_instruct_columns].head())\n",
    "\n",
    "import numpy as np\n",
    "# Count non-NaN values in these columns for each row\n",
    "non_nan_counts = file_df[principle_instruct_columns].notna().sum(axis=1)\n",
    "np.mean(non_nan_counts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm-spri",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
