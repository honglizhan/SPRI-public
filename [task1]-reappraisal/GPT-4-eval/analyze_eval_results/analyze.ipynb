{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30 30\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "import pandas as pd\n",
    "\n",
    "model_name = \"mixtral-8x7b-instruct-v01\"\n",
    "\n",
    "# file_path = f\"eval_outputs/{model_name}/[RESORT_baselines]/ITER/+cons_principle_instruct\"\n",
    "# file_path = f\"eval_outputs/{model_name}/[seed=Principle_Instruct]/[resort_human_eval_30]-generated_refined_responses\"\n",
    "# file_path = f\"eval_outputs/{model_name}/[seed=RESORT_constitutions]/[resort_human_eval_30]-generated_refined_responses\"\n",
    "file_path = f\"eval_outputs/{model_name}/[seed=1_RESORT_constitutions_per_dimension]/[resort_human_eval_30]-generated_refined_responses\"\n",
    "# file_path = f\"eval_outputs/{model_name}/[seed=none]/[resort_human_eval_30]-generated_refined_responses\"\n",
    "\n",
    "\n",
    "eval_1_standard_alignment = pd.read_json(f\"../{file_path}/eval-1_standard_alignment.jsonl\", lines=True).drop_duplicates(subset=[\"Reddit ID\", \"appraisal_dimension_id\"])\n",
    "eval_2_empathy = pd.read_json(f\"../{file_path}/eval-2_empathy.jsonl\", lines=True).drop_duplicates(subset=[\"Reddit ID\", \"appraisal_dimension_id\"])\n",
    "eval_3_harmful = pd.read_json(f\"../{file_path}/eval-3_harmful.jsonl\", lines=True).drop_duplicates(subset=[\"Reddit ID\", \"appraisal_dimension_id\"])\n",
    "eval_4_factuality = pd.read_json(f\"../{file_path}/eval-4_factuality.jsonl\", lines=True).drop_duplicates(subset=[\"Reddit ID\", \"appraisal_dimension_id\"])\n",
    "\n",
    "print (len(eval_1_standard_alignment), len(eval_2_empathy), len(eval_3_harmful), len(eval_4_factuality))\n",
    "assert len(eval_1_standard_alignment) == len(eval_2_empathy) == len(eval_3_harmful) == len(eval_4_factuality) == 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [eval_1_standard_alignment, eval_2_empathy, eval_3_harmful, eval_4_factuality]:\n",
    "    raw_output_cols = [col for col in df.columns if col.startswith('evaluation_raw_output-')]\n",
    "    df['evaluation_raw_output'] = df[raw_output_cols].bfill(axis=1).iloc[:, 0]\n",
    "    df = df.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_1_standard_alignment[\"evaluation_raw_output_score\"] = eval_1_standard_alignment[\"evaluation_raw_output\"].apply(utils.extract_score_criterion_1_standard_alignment)\n",
    "eval_2_empathy[\"evaluation_raw_output_score\"] = eval_2_empathy[\"evaluation_raw_output\"].apply(utils.extract_score_criterion_2_empathy)\n",
    "eval_3_harmful[\"evaluation_raw_output_score\"] = eval_3_harmful[\"evaluation_raw_output\"].apply(utils.extract_score_criterion_3_harmful)\n",
    "eval_4_factuality[\"evaluation_raw_output_score\"] = eval_4_factuality[\"evaluation_raw_output\"].apply(utils.extract_score_criterion_4_factuality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_1_standard_alignment = eval_1_standard_alignment.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)\n",
    "eval_2_empathy = eval_2_empathy.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)\n",
    "eval_3_harmful = eval_3_harmful.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)\n",
    "eval_4_factuality = eval_4_factuality.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mModel: mixtral-8x7b-instruct-v01\u001b[0m\n",
      "\u001b[33mFile Path: eval_outputs/mixtral-8x7b-instruct-v01/[seed=1_RESORT_constitutions_per_dimension]/[resort_human_eval_30]-generated_refined_responses\u001b[0m\n",
      "\u001b[32mStandard Alignment: 8.033333333333333\u001b[0m\n",
      "\u001b[32mEmpathy: 4.766666666666667\u001b[0m\n",
      "\u001b[32mHarmful: 0.0\u001b[0m\n",
      "\u001b[32mFactuality: 0.9333333333333333\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "\n",
    "print (colored(f\"Model: {model_name}\", \"red\"))\n",
    "print (colored(f\"File Path: {file_path}\", \"yellow\"))\n",
    "\n",
    "print (colored(f\"Standard Alignment: {eval_1_standard_alignment['evaluation_raw_output_score'].mean()}\", \"green\"))\n",
    "print (colored(f\"Empathy: {eval_2_empathy['evaluation_raw_output_score'].mean()}\", \"green\"))\n",
    "print (colored(f\"Harmful: {eval_3_harmful['evaluation_raw_output_score'].mean()}\", \"green\"))\n",
    "print (colored(f\"Factuality: {eval_4_factuality['evaluation_raw_output_score'].mean()}\", \"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measure significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"self-refine\" in file_path or \"vanilla\" in file_path:\n",
    "    raise SystemError(\"We don't evaluate the baseline models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30 30\n"
     ]
    }
   ],
   "source": [
    "### ------ vanilla baseline\n",
    "vanilla_baseline_path = f\"eval_outputs/{model_name}/[RESORT_baselines]/INDV/vanilla\"\n",
    "\n",
    "vanilla_baseline_eval_1_standard_alignment = pd.read_json(f\"../{vanilla_baseline_path}/eval-1_standard_alignment.jsonl\", lines=True).drop_duplicates()\n",
    "vanilla_baseline_eval_2_empathy = pd.read_json(f\"../{vanilla_baseline_path}/eval-2_empathy.jsonl\", lines=True).drop_duplicates()\n",
    "vanilla_baseline_eval_3_harmful = pd.read_json(f\"../{vanilla_baseline_path}/eval-3_harmful.jsonl\", lines=True).drop_duplicates()\n",
    "vanilla_baseline_eval_4_factuality = pd.read_json(f\"../{vanilla_baseline_path}/eval-4_factuality.jsonl\", lines=True).drop_duplicates(subset=[\"Reddit ID\", \"appraisal_dimension_id\"])\n",
    "\n",
    "print (len(vanilla_baseline_eval_1_standard_alignment), len(vanilla_baseline_eval_2_empathy), len(vanilla_baseline_eval_3_harmful), len(vanilla_baseline_eval_4_factuality))\n",
    "assert len(vanilla_baseline_eval_1_standard_alignment) == len(vanilla_baseline_eval_2_empathy) == len(vanilla_baseline_eval_3_harmful) == len(vanilla_baseline_eval_4_factuality) == 30\n",
    "\n",
    "for df in [\n",
    "    vanilla_baseline_eval_1_standard_alignment, vanilla_baseline_eval_2_empathy,\n",
    "    vanilla_baseline_eval_3_harmful, vanilla_baseline_eval_4_factuality]:\n",
    "    raw_output_cols = [col for col in df.columns if col.startswith('evaluation_raw_output-')]\n",
    "    df['evaluation_raw_output'] = df[raw_output_cols].bfill(axis=1).iloc[:, 0]\n",
    "    df = df.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)\n",
    "\n",
    "vanilla_baseline_eval_1_standard_alignment[\"evaluation_raw_output_score\"] = vanilla_baseline_eval_1_standard_alignment[\"evaluation_raw_output\"].apply(utils.extract_score_criterion_1_standard_alignment)\n",
    "vanilla_baseline_eval_2_empathy[\"evaluation_raw_output_score\"] = vanilla_baseline_eval_2_empathy[\"evaluation_raw_output\"].apply(utils.extract_score_criterion_2_empathy)\n",
    "vanilla_baseline_eval_3_harmful[\"evaluation_raw_output_score\"] = vanilla_baseline_eval_3_harmful[\"evaluation_raw_output\"].apply(utils.extract_score_criterion_3_harmful)\n",
    "vanilla_baseline_eval_4_factuality[\"evaluation_raw_output_score\"] = vanilla_baseline_eval_4_factuality[\"evaluation_raw_output\"].apply(utils.extract_score_criterion_4_factuality)\n",
    "\n",
    "vanilla_baseline_eval_1_standard_alignment = vanilla_baseline_eval_1_standard_alignment.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)\n",
    "vanilla_baseline_eval_2_empathy = vanilla_baseline_eval_2_empathy.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)\n",
    "vanilla_baseline_eval_3_harmful = vanilla_baseline_eval_3_harmful.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)\n",
    "vanilla_baseline_eval_4_factuality = vanilla_baseline_eval_4_factuality.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30 30 30 30\n"
     ]
    }
   ],
   "source": [
    "### ------ self refine baseline\n",
    "self_refine_baseline_path = f\"eval_outputs/{model_name}/[RESORT_baselines]/ITER/self-refine\"\n",
    "\n",
    "self_refine_baseline_eval_1_standard_alignment = pd.read_json(f\"../{self_refine_baseline_path}/eval-1_standard_alignment.jsonl\", lines=True).drop_duplicates()\n",
    "self_refine_baseline_eval_2_empathy = pd.read_json(f\"../{self_refine_baseline_path}/eval-2_empathy.jsonl\", lines=True).drop_duplicates()\n",
    "self_refine_baseline_eval_3_harmful = pd.read_json(f\"../{self_refine_baseline_path}/eval-3_harmful.jsonl\", lines=True).drop_duplicates()\n",
    "self_refine_baseline_eval_4_factuality = pd.read_json(f\"../{self_refine_baseline_path}/eval-4_factuality.jsonl\", lines=True).drop_duplicates(subset=[\"Reddit ID\", \"appraisal_dimension_id\"])\n",
    "\n",
    "print (len(self_refine_baseline_eval_1_standard_alignment), len(self_refine_baseline_eval_2_empathy), len(self_refine_baseline_eval_3_harmful), len(self_refine_baseline_eval_4_factuality))\n",
    "assert len(self_refine_baseline_eval_1_standard_alignment) == len(self_refine_baseline_eval_2_empathy) == len(self_refine_baseline_eval_3_harmful) == len(self_refine_baseline_eval_4_factuality) == 30\n",
    "\n",
    "for df in [\n",
    "    self_refine_baseline_eval_1_standard_alignment, self_refine_baseline_eval_2_empathy,\n",
    "    self_refine_baseline_eval_3_harmful, self_refine_baseline_eval_4_factuality]:\n",
    "    raw_output_cols = [col for col in df.columns if col.startswith('evaluation_raw_output-')]\n",
    "    df['evaluation_raw_output'] = df[raw_output_cols].bfill(axis=1).iloc[:, 0]\n",
    "    df = df.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)\n",
    "\n",
    "self_refine_baseline_eval_1_standard_alignment[\"evaluation_raw_output_score\"] = self_refine_baseline_eval_1_standard_alignment[\"evaluation_raw_output\"].apply(utils.extract_score_criterion_1_standard_alignment)\n",
    "self_refine_baseline_eval_2_empathy[\"evaluation_raw_output_score\"] = self_refine_baseline_eval_2_empathy[\"evaluation_raw_output\"].apply(utils.extract_score_criterion_2_empathy)\n",
    "self_refine_baseline_eval_3_harmful[\"evaluation_raw_output_score\"] = self_refine_baseline_eval_3_harmful[\"evaluation_raw_output\"].apply(utils.extract_score_criterion_3_harmful)\n",
    "self_refine_baseline_eval_4_factuality[\"evaluation_raw_output_score\"] = self_refine_baseline_eval_4_factuality[\"evaluation_raw_output\"].apply(utils.extract_score_criterion_4_factuality)\n",
    "\n",
    "self_refine_baseline_eval_1_standard_alignment = self_refine_baseline_eval_1_standard_alignment.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)\n",
    "self_refine_baseline_eval_2_empathy = self_refine_baseline_eval_2_empathy.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)\n",
    "self_refine_baseline_eval_3_harmful = self_refine_baseline_eval_3_harmful.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)\n",
    "self_refine_baseline_eval_4_factuality = self_refine_baseline_eval_4_factuality.sort_values(by=[\"Reddit ID\", \"appraisal_dimension_id\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mModel: mixtral-8x7b-instruct-v01\u001b[0m\n",
      "\u001b[32mMethod: eval_outputs/mixtral-8x7b-instruct-v01/[seed=1_RESORT_constitutions_per_dimension]/[resort_human_eval_30]-generated_refined_responses\u001b[0m\n",
      "1. Standard Alignment: 8.03*^\n",
      "2. Empathy: 4.77*^\n",
      "3. Harmful: 0.0\n",
      "4. Factuality: 0.93^\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "print (colored(f\"Model: {model_name}\", \"red\"))\n",
    "print (colored(f\"Method: {file_path}\", \"green\"))\n",
    "\n",
    "def p_value_sign(p_value, sign=\"*\"):\n",
    "    if p_value < 0.05:\n",
    "        return sign\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "assert self_refine_baseline_eval_1_standard_alignment[[\"Reddit ID\", \"appraisal_dimension_id\"]].equals(eval_1_standard_alignment[[\"Reddit ID\", \"appraisal_dimension_id\"]]), \"The DataFrames are not equal.\"\n",
    "assert vanilla_baseline_eval_1_standard_alignment[[\"Reddit ID\", \"appraisal_dimension_id\"]].equals(eval_1_standard_alignment[[\"Reddit ID\", \"appraisal_dimension_id\"]]), \"The DataFrames are not equal.\"\n",
    "t, p_vanilla = ttest_rel(eval_1_standard_alignment[\"evaluation_raw_output_score\"], vanilla_baseline_eval_1_standard_alignment[\"evaluation_raw_output_score\"])\n",
    "t, p_self_refine = ttest_rel(eval_1_standard_alignment[\"evaluation_raw_output_score\"], self_refine_baseline_eval_1_standard_alignment[\"evaluation_raw_output_score\"])\n",
    "print (f\"1. Standard Alignment: {round(eval_1_standard_alignment['evaluation_raw_output_score'].mean(), 2)}\" + p_value_sign(p_vanilla, \"*\") + p_value_sign(p_self_refine, \"^\"))\n",
    "\n",
    "assert self_refine_baseline_eval_2_empathy[[\"Reddit ID\", \"appraisal_dimension_id\"]].equals(eval_2_empathy[[\"Reddit ID\", \"appraisal_dimension_id\"]]), \"The DataFrames are not equal.\"\n",
    "assert vanilla_baseline_eval_2_empathy[[\"Reddit ID\", \"appraisal_dimension_id\"]].equals(eval_2_empathy[[\"Reddit ID\", \"appraisal_dimension_id\"]]), \"The DataFrames are not equal.\"\n",
    "t, p_vanilla = ttest_rel(eval_2_empathy[\"evaluation_raw_output_score\"], vanilla_baseline_eval_2_empathy[\"evaluation_raw_output_score\"])\n",
    "t, p_self_refine = ttest_rel(eval_2_empathy[\"evaluation_raw_output_score\"], self_refine_baseline_eval_2_empathy[\"evaluation_raw_output_score\"])\n",
    "print (f\"2. Empathy: {round(eval_2_empathy['evaluation_raw_output_score'].mean(), 2)}\" + p_value_sign(p_vanilla, \"*\") + p_value_sign(p_self_refine, \"^\"))\n",
    "\n",
    "assert self_refine_baseline_eval_3_harmful[[\"Reddit ID\", \"appraisal_dimension_id\"]].equals(eval_3_harmful[[\"Reddit ID\", \"appraisal_dimension_id\"]]), \"The DataFrames are not equal.\"\n",
    "assert vanilla_baseline_eval_3_harmful[[\"Reddit ID\", \"appraisal_dimension_id\"]].equals(eval_3_harmful[[\"Reddit ID\", \"appraisal_dimension_id\"]]), \"The DataFrames are not equal.\"\n",
    "t, p_vanilla = ttest_rel(eval_3_harmful[\"evaluation_raw_output_score\"], vanilla_baseline_eval_3_harmful[\"evaluation_raw_output_score\"])\n",
    "t, p_self_refine = ttest_rel(eval_3_harmful[\"evaluation_raw_output_score\"], self_refine_baseline_eval_3_harmful[\"evaluation_raw_output_score\"])\n",
    "print (f\"3. Harmful: {round(eval_3_harmful['evaluation_raw_output_score'].mean(), 2)}\" + p_value_sign(p_vanilla, \"*\") + p_value_sign(p_self_refine, \"^\"))\n",
    "\n",
    "assert self_refine_baseline_eval_4_factuality[[\"Reddit ID\", \"appraisal_dimension_id\"]].equals(eval_4_factuality[[\"Reddit ID\", \"appraisal_dimension_id\"]]), \"The DataFrames are not equal.\"\n",
    "assert vanilla_baseline_eval_4_factuality[[\"Reddit ID\", \"appraisal_dimension_id\"]].equals(eval_4_factuality[[\"Reddit ID\", \"appraisal_dimension_id\"]]), \"The DataFrames are not equal.\"\n",
    "t, p_vanilla = ttest_rel(eval_4_factuality[\"evaluation_raw_output_score\"], vanilla_baseline_eval_4_factuality[\"evaluation_raw_output_score\"])\n",
    "t, p_self_refine = ttest_rel(eval_4_factuality[\"evaluation_raw_output_score\"], self_refine_baseline_eval_4_factuality[\"evaluation_raw_output_score\"])\n",
    "print (f\"4. Factuality: {round(eval_4_factuality['evaluation_raw_output_score'].mean(), 2)}\" + p_value_sign(p_vanilla, \"*\") + p_value_sign(p_self_refine, \"^\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
